{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from spacy.lang.en.examples import sentences \n",
    "import spacy as sp\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('stopwords')\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline\n",
    "\n",
    "# import zipfile\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('data/TrumpTweetsCleaned.csv')\n",
    "#09-11-2017 to 02-16-2018\n",
    "before = pd.to_datetime('2017-09-11')\n",
    "after = pd.to_datetime('2018-02-16')\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (James)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing functions for the dataframe\n",
    "def delete_punct(text):\n",
    "    '''\n",
    "    removes special characters from the document\n",
    "    '''\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct \n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    removes all stopwords according to the pre-built english dictionary of known stopwords\n",
    "    '''\n",
    "    words = [w for w in text if w not in set(stopwords.words('english'))]\n",
    "    return words\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    returns the lemmas of each word in the document\n",
    "    '''\n",
    "    lemmatized = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lemmatized\n",
    "def joiner(text):\n",
    "    '''\n",
    "    joins the comma-separated list into one string\n",
    "    '''\n",
    "    joined = \" \".join([i for i in text])\n",
    "    return joined\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_pipeline(tweets):\n",
    "    '''\n",
    "    The preprocessing pipline applied to each document in the dataframe\n",
    "    '''\n",
    "    tweets['preproc'] = tweets['text'].apply(lambda x: delete_punct(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: remove_stopwords(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: lemmatize(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: joiner(x))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = preproc_pipeline(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting sk-learn's TF-IDF Vectorizer to our dataframe and returning list of most important keywords\n",
    "docs = tweets['preproc'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features = 2500, min_df = 10, max_df = .9)\n",
    "tfidf2 = TfidfVectorizer(max_features = 150, min_df = 160, max_df = .9)\n",
    "X = tfidf.fit_transform(docs).toarray()\n",
    "feature_names = tfidf.get_feature_names()\n",
    "X2 = tfidf2.fit_transform(docs).toarray()\n",
    "features_names2 = tfidf2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(l1,l2):\n",
    "    '''\n",
    "    finding intersection of elements between two lists\n",
    "    '''\n",
    "    return list(set(l1) & set(l2))\n",
    "def term_extract(df):\n",
    "    '''\n",
    "    Will return a column of important keywords in each document determined by TF-IDF \n",
    "    Sentiment Analysis via Textblob will only be run on these keywords \n",
    "    '''\n",
    "    df['keywords'] = df['preproc'].apply(lambda x: intersection(x.split(' '), feature_names))\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: joiner(x))\n",
    "    return df\n",
    "tweets = term_extract(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['polarity'] = [TextBlob(tweets['keywords'].values[i]).polarity for i in range(len(tweets))]\n",
    "tweets['subjectivity'] = [TextBlob(tweets['keywords'].values[i]).subjectivity for i in range(len(tweets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns \n",
    "tweets  = tweets.drop(columns = ['text', 'preproc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv('data/SP500_intraday.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['Time_pd'] = pd.to_timedelta(stock['Time']+':00')\n",
    "stock['Date_time'] = pd.to_datetime(pd.to_datetime(stock['Date'])+ pd.to_timedelta(stock['Time_pd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_time = stock['Date_time'].iloc[-1]\n",
    "earliest_time = stock['Date_time'][0] \n",
    "tweets = tweets[(tweets['created_at'] > earliest_time) & (tweets['created_at'] < latest_time)]\n",
    "tweets = tweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edit the minute after\n",
    "time_min = 5\n",
    "time_after = tweets['created_at'] + pd.to_timedelta(time_min, unit = 'm')\n",
    "time_of = tweets['created_at']\n",
    "good_time_after = sorted(list(set(stock['Date_time']) & set(time_after)))\n",
    "x = np.arange(0,len(good_time_after))\n",
    "tweets_stock_viable = tweets.iloc[np.concatenate([np.where(time_after == good_time_after[x])[0] for x in x])]\n",
    "\n",
    "time_after2 = tweets_stock_viable['created_at'] + pd.to_timedelta(time_min, unit = 'm')\n",
    "time_of2 = tweets_stock_viable['created_at']\n",
    "x = time_of2\n",
    "\n",
    "bad_var = [stock[stock['Date_time'] == time_of2[i]]['Open'].values\n",
    "             for i in tweets_stock_viable.index]\n",
    "bad = np.where(pd.DataFrame(bad_var, index = tweets_stock_viable.index).isna())[0]\n",
    "tweets_stock_viable = tweets_stock_viable.drop(tweets_stock_viable.index[bad])\n",
    "stock_df = [stock[stock['Date_time'] == time_after2[i]]['Close'].values - stock[stock['Date_time'] == time_of2[i]]['Open'].values\n",
    "             for i in x.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable['stock_dif'] = pd.DataFrame(np.concatenate(stock_df), index= tweets_stock_viable.index)\n",
    "length = len(tweets_stock_viable)\n",
    "tweets_stock_viable['stock_up'] = [1 if tweets_stock_viable['stock_dif'][i] > 0 else 0 for i in tweets_stock_viable.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_stock_viable = tweets_stock_viable.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable = pd.read_csv('data/tweets_stock_viable_5')\n",
    "tweets_stock_viable['keywords'] = tweets_stock_viable['keywords'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "tweets = [[word.lower() for word in sent.split() if word not in remove_terms] for sent in tweets_stock_viable['keywords']]\n",
    "tweets = [' '.join(tok_sent) for tok_sent in tweets]\n",
    "tweets = filter(None, normalize_corpus(tweets))\n",
    "tweets = [tok_sent for tok_sent in tweets if len(tok_sent.split()) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in tweets]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)\n",
    "\n",
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['china', 'tariff', 'democrat', 'farmer', 'trade', 'war', 'xi','president']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_similarity(word):\n",
    "    succ = []\n",
    "    for i in range(len(tweets_stock_viable)):\n",
    "        similarities = []\n",
    "        phrase = tweets_stock_viable['keywords'][i].lower()\n",
    "        phrase = re.sub(r'[^a-zA-Z\\s]', '', phrase, re.I|re.A)\n",
    "        phrase = re.sub(r'http\\S+', '', phrase)\n",
    "        for w in phrase:\n",
    "            if w in w2v_model.wv.vocab:\n",
    "                similarities.append(w2v_model.wv.n_similarity([word], [w]))\n",
    "        if similarities == []:\n",
    "            succ.append(0)\n",
    "        else:\n",
    "            succ.append(max(similarities) + np.mean(similarities))\n",
    "    tweets_stock_viable['{}_similarity'.format(word)] = succ\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big problem here. Similarities with this self-made corpus seem to be always the same for 25/50/75 quantiles if you check the tweets_stock_viable.describe() output\n",
    "\n",
    "# I don't know why this happens? I added the mean of the similarities of each word in each tweet to the similarity value appended to the df to give more variability but this underlying problem is still there. Also it doesn't improve our predictions whatsoever basically, even using RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['china', 'tariff', 'trade', 'war', 'xi', 'intellectual', 'property', 'farm', 'soy', 'fentanyl', 'fed']\n",
    "volfefe_vocab = ['china', 'xi', 'intellectual', 'soy', 'fentanyl', 'billion',  'products', 'fed',  'democrats',  'great',  'dollars',  'tariff',  'country',  'mueller',  'border', 'president', 'congressman', 'people', 'korea', 'years', 'farmers', 'going', 'trade', 'never']\n",
    "for word in volfefe_vocab:\n",
    "    add_similarity(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
