{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from spacy.lang.en.examples import sentences \n",
    "import spacy as sp\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('stopwords')\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline\n",
    "\n",
    "# import zipfile\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('data/TrumpTweetsCleaned.csv')\n",
    "#09-11-2017 to 02-16-2018\n",
    "before = pd.to_datetime('2017-09-11')\n",
    "after = pd.to_datetime('2018-02-16')\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (James)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing functions for the dataframe\n",
    "def delete_punct(text):\n",
    "    '''\n",
    "    removes special characters from the document\n",
    "    '''\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct \n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    removes all stopwords according to the pre-built english dictionary of known stopwords\n",
    "    '''\n",
    "    words = [w for w in text if w not in set(stopwords.words('english'))]\n",
    "    return words\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    returns the lemmas of each word in the document\n",
    "    '''\n",
    "    lemmatized = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lemmatized\n",
    "def joiner(text):\n",
    "    '''\n",
    "    joins the comma-separated list into one string\n",
    "    '''\n",
    "    joined = \" \".join([i for i in text])\n",
    "    return joined\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_pipeline(tweets):\n",
    "    '''\n",
    "    The preprocessing pipline applied to each document in the dataframe\n",
    "    '''\n",
    "    tweets['preproc'] = tweets['text'].apply(lambda x: delete_punct(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: remove_stopwords(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: lemmatize(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: joiner(x))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = preproc_pipeline(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting sk-learn's TF-IDF Vectorizer to our dataframe and returning list of most important keywords\n",
    "docs = tweets['preproc'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features = 2500, min_df = 10, max_df = .9)\n",
    "tfidf2 = TfidfVectorizer(max_features = 150, min_df = 160, max_df = .9)\n",
    "X = tfidf.fit_transform(docs).toarray()\n",
    "feature_names = tfidf.get_feature_names()\n",
    "X2 = tfidf2.fit_transform(docs).toarray()\n",
    "features_names2 = tfidf2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(l1,l2):\n",
    "    '''\n",
    "    finding intersection of elements between two lists\n",
    "    '''\n",
    "    return list(set(l1) & set(l2))\n",
    "def term_extract(df):\n",
    "    '''\n",
    "    Will return a column of important keywords in each document determined by TF-IDF \n",
    "    Sentiment Analysis via Textblob will only be run on these keywords \n",
    "    '''\n",
    "    df['keywords'] = df['preproc'].apply(lambda x: intersection(x.split(' '), feature_names))\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: joiner(x))\n",
    "    return df\n",
    "tweets = term_extract(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['polarity'] = [TextBlob(tweets['keywords'].values[i]).polarity for i in range(len(tweets))]\n",
    "tweets['subjectivity'] = [TextBlob(tweets['keywords'].values[i]).subjectivity for i in range(len(tweets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns \n",
    "tweets  = tweets.drop(columns = ['text', 'preproc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv('data/SP500_intraday.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['Time_pd'] = pd.to_timedelta(stock['Time']+':00')\n",
    "stock['Date_time'] = pd.to_datetime(pd.to_datetime(stock['Date'])+ pd.to_timedelta(stock['Time_pd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_time = stock['Date_time'].iloc[-1]\n",
    "earliest_time = stock['Date_time'][0] \n",
    "tweets = tweets[(tweets['created_at'] > earliest_time) & (tweets['created_at'] < latest_time)]\n",
    "tweets = tweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edit the minute after\n",
    "time_min = 5\n",
    "time_after = tweets['created_at'] + pd.to_timedelta(time_min, unit = 'm')\n",
    "time_of = tweets['created_at']\n",
    "good_time_after = sorted(list(set(stock['Date_time']) & set(time_after)))\n",
    "x = np.arange(0,len(good_time_after))\n",
    "tweets_stock_viable = tweets.iloc[np.concatenate([np.where(time_after == good_time_after[x])[0] for x in x])]\n",
    "\n",
    "time_after2 = tweets_stock_viable['created_at'] + pd.to_timedelta(time_min, unit = 'm')\n",
    "time_of2 = tweets_stock_viable['created_at']\n",
    "x = time_of2\n",
    "\n",
    "bad_var = [stock[stock['Date_time'] == time_of2[i]]['Open'].values\n",
    "             for i in tweets_stock_viable.index]\n",
    "bad = np.where(pd.DataFrame(bad_var, index = tweets_stock_viable.index).isna())[0]\n",
    "tweets_stock_viable = tweets_stock_viable.drop(tweets_stock_viable.index[bad])\n",
    "stock_df = [stock[stock['Date_time'] == time_after2[i]]['Close'].values - stock[stock['Date_time'] == time_of2[i]]['Open'].values\n",
    "             for i in x.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable['stock_dif'] = pd.DataFrame(np.concatenate(stock_df), index= tweets_stock_viable.index)\n",
    "length = len(tweets_stock_viable)\n",
    "tweets_stock_viable['stock_up'] = [1 if tweets_stock_viable['stock_dif'][i] > 0 else 0 for i in tweets_stock_viable.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable.to_csv('data/tweets_stock_viable_5', index = False)\n",
    "tweets_stock_viable = tweets_stock_viable.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable = pd.read_csv('data/tweets_stock_viable_5')\n",
    "tweets_stock_viable['keywords'] = tweets_stock_viable['keywords'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "tweets = [[word.lower() for word in sent.split() if word not in remove_terms] for sent in tweets_stock_viable['keywords']]\n",
    "tweets = [' '.join(tok_sent) for tok_sent in tweets]\n",
    "tweets = filter(None, normalize_corpus(tweets))\n",
    "tweets = [tok_sent for tok_sent in tweets if len(tok_sent.split()) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in tweets]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)\n",
    "\n",
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['china', 'tariff', 'democrat', 'farmer', 'trade', 'war', 'xi','president']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_similarity(word):\n",
    "    succ = []\n",
    "    for i in range(len(tweets_stock_viable)):\n",
    "        similarities = []\n",
    "        phrase = tweets_stock_viable['keywords'][i].lower()\n",
    "        phrase = re.sub(r'[^a-zA-Z\\s]', '', phrase, re.I|re.A)\n",
    "        phrase = re.sub(r'http\\S+', '', phrase)\n",
    "        for w in phrase:\n",
    "            if w in w2v_model.wv.vocab:\n",
    "                similarities.append(w2v_model.wv.n_similarity([word], [w]))\n",
    "        if similarities == []:\n",
    "            succ.append(0)\n",
    "        else:\n",
    "            succ.append(max(similarities) + np.mean(similarities))\n",
    "    tweets_stock_viable['{}_similarity'.format(word)] = succ\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big problem here. Similarities with this self-made corpus seem to be always the same for 25/50/75 quantiles if you check the tweets_stock_viable.describe() output\n",
    "\n",
    "# I don't know why this happens? I added the mean of the similarities of each word in each tweet to the similarity value appended to the df to give more variability but this underlying problem is still there. Also it doesn't improve our predictions whatsoever basically, even using RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['china', 'tariff', 'trade', 'war', 'xi', 'intellectual', 'property', 'farm', 'soy', 'fentanyl', 'fed']\n",
    "volfefe_vocab = ['china', 'xi', 'intellectual', 'soy', 'fentanyl', 'billion',  'products', 'fed',  'democrats',  'great',  'dollars',  'tariff',  'country',  'mueller',  'border', 'president', 'congressman', 'people', 'korea', 'years', 'farmers', 'going', 'trade', 'never']\n",
    "for word in volfefe_vocab:\n",
    "    add_similarity(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4149 samples, validate on 1038 samples\n",
      "Epoch 1/50\n",
      "4149/4149 [==============================] - 1s 196us/sample - loss: 0.7127 - acc: 0.5343 - val_loss: 0.7048 - val_acc: 0.5636\n",
      "Epoch 2/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6898 - acc: 0.5563 - val_loss: 0.6966 - val_acc: 0.5617\n",
      "Epoch 3/50\n",
      "4149/4149 [==============================] - 0s 95us/sample - loss: 0.6860 - acc: 0.5613 - val_loss: 0.6995 - val_acc: 0.5501\n",
      "Epoch 4/50\n",
      "4149/4149 [==============================] - 0s 91us/sample - loss: 0.6839 - acc: 0.5599 - val_loss: 0.6956 - val_acc: 0.5453\n",
      "Epoch 5/50\n",
      "4149/4149 [==============================] - 0s 93us/sample - loss: 0.6817 - acc: 0.5664 - val_loss: 0.6976 - val_acc: 0.5453\n",
      "Epoch 6/50\n",
      "4149/4149 [==============================] - 0s 93us/sample - loss: 0.6808 - acc: 0.5633 - val_loss: 0.6976 - val_acc: 0.5414\n",
      "Epoch 7/50\n",
      "4149/4149 [==============================] - 0s 89us/sample - loss: 0.6791 - acc: 0.5736 - val_loss: 0.6996 - val_acc: 0.5308\n",
      "Epoch 8/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6785 - acc: 0.5744 - val_loss: 0.6975 - val_acc: 0.5347\n",
      "Epoch 9/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6773 - acc: 0.5722 - val_loss: 0.6990 - val_acc: 0.5376\n",
      "Epoch 10/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6772 - acc: 0.5741 - val_loss: 0.6991 - val_acc: 0.5231\n",
      "Epoch 11/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6760 - acc: 0.5765 - val_loss: 0.7009 - val_acc: 0.5356\n",
      "Epoch 12/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6750 - acc: 0.5753 - val_loss: 0.7000 - val_acc: 0.5279\n",
      "Epoch 13/50\n",
      "4149/4149 [==============================] - 0s 89us/sample - loss: 0.6746 - acc: 0.5763 - val_loss: 0.7006 - val_acc: 0.5279\n",
      "Epoch 14/50\n",
      "4149/4149 [==============================] - 0s 89us/sample - loss: 0.6741 - acc: 0.5760 - val_loss: 0.7002 - val_acc: 0.5328\n",
      "Epoch 15/50\n",
      "4149/4149 [==============================] - 0s 91us/sample - loss: 0.6734 - acc: 0.5770 - val_loss: 0.7010 - val_acc: 0.5231\n",
      "Epoch 16/50\n",
      "4149/4149 [==============================] - 0s 93us/sample - loss: 0.6729 - acc: 0.5782 - val_loss: 0.6991 - val_acc: 0.5260\n",
      "Epoch 17/50\n",
      "4149/4149 [==============================] - 0s 91us/sample - loss: 0.6727 - acc: 0.5792 - val_loss: 0.7033 - val_acc: 0.5193\n",
      "Epoch 18/50\n",
      "4149/4149 [==============================] - 0s 95us/sample - loss: 0.6718 - acc: 0.5850 - val_loss: 0.7000 - val_acc: 0.5250\n",
      "Epoch 19/50\n",
      "4149/4149 [==============================] - 0s 93us/sample - loss: 0.6716 - acc: 0.5816 - val_loss: 0.6999 - val_acc: 0.5337\n",
      "Epoch 20/50\n",
      "4149/4149 [==============================] - 0s 88us/sample - loss: 0.6711 - acc: 0.5852 - val_loss: 0.7021 - val_acc: 0.5318\n",
      "Epoch 21/50\n",
      "4149/4149 [==============================] - 0s 93us/sample - loss: 0.6700 - acc: 0.5828 - val_loss: 0.7011 - val_acc: 0.5279\n",
      "Epoch 22/50\n",
      "4149/4149 [==============================] - 0s 93us/sample - loss: 0.6700 - acc: 0.5879 - val_loss: 0.7054 - val_acc: 0.5250\n",
      "Epoch 23/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6695 - acc: 0.5835 - val_loss: 0.7001 - val_acc: 0.5299\n",
      "Epoch 24/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6685 - acc: 0.5854 - val_loss: 0.7055 - val_acc: 0.5347\n",
      "Epoch 25/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6676 - acc: 0.5852 - val_loss: 0.7072 - val_acc: 0.5356\n",
      "Epoch 26/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6693 - acc: 0.5864 - val_loss: 0.7056 - val_acc: 0.5270\n",
      "Epoch 27/50\n",
      "4149/4149 [==============================] - 0s 95us/sample - loss: 0.6679 - acc: 0.5871 - val_loss: 0.7067 - val_acc: 0.5106\n",
      "Epoch 28/50\n",
      "4149/4149 [==============================] - 0s 98us/sample - loss: 0.6670 - acc: 0.5874 - val_loss: 0.7051 - val_acc: 0.5154\n",
      "Epoch 29/50\n",
      "4149/4149 [==============================] - 0s 98us/sample - loss: 0.6662 - acc: 0.5936 - val_loss: 0.7076 - val_acc: 0.5222\n",
      "Epoch 30/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6656 - acc: 0.5927 - val_loss: 0.7079 - val_acc: 0.5260\n",
      "Epoch 31/50\n",
      "4149/4149 [==============================] - 0s 104us/sample - loss: 0.6657 - acc: 0.5893 - val_loss: 0.7069 - val_acc: 0.5193\n",
      "Epoch 32/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6647 - acc: 0.5879 - val_loss: 0.7075 - val_acc: 0.5270\n",
      "Epoch 33/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6644 - acc: 0.5936 - val_loss: 0.7064 - val_acc: 0.5145\n",
      "Epoch 34/50\n",
      "4149/4149 [==============================] - 0s 102us/sample - loss: 0.6644 - acc: 0.5883 - val_loss: 0.7090 - val_acc: 0.5145\n",
      "Epoch 35/50\n",
      "4149/4149 [==============================] - 0s 104us/sample - loss: 0.6642 - acc: 0.5922 - val_loss: 0.7093 - val_acc: 0.5289\n",
      "Epoch 36/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6634 - acc: 0.5997 - val_loss: 0.7099 - val_acc: 0.5145\n",
      "Epoch 37/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6626 - acc: 0.5907 - val_loss: 0.7098 - val_acc: 0.5154\n",
      "Epoch 38/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6623 - acc: 0.5953 - val_loss: 0.7120 - val_acc: 0.5135\n",
      "Epoch 39/50\n",
      "4149/4149 [==============================] - 0s 94us/sample - loss: 0.6617 - acc: 0.5953 - val_loss: 0.7072 - val_acc: 0.5260\n",
      "Epoch 40/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6614 - acc: 0.5982 - val_loss: 0.7107 - val_acc: 0.5222\n",
      "Epoch 41/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6609 - acc: 0.6004 - val_loss: 0.7162 - val_acc: 0.5183\n",
      "Epoch 42/50\n",
      "4149/4149 [==============================] - 0s 96us/sample - loss: 0.6614 - acc: 0.5941 - val_loss: 0.7173 - val_acc: 0.5164\n",
      "Epoch 43/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6601 - acc: 0.5953 - val_loss: 0.7125 - val_acc: 0.5279\n",
      "Epoch 44/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6594 - acc: 0.6028 - val_loss: 0.7197 - val_acc: 0.5222\n",
      "Epoch 45/50\n",
      "4149/4149 [==============================] - 0s 96us/sample - loss: 0.6592 - acc: 0.6006 - val_loss: 0.7123 - val_acc: 0.5366\n",
      "Epoch 46/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6588 - acc: 0.5939 - val_loss: 0.7133 - val_acc: 0.5299\n",
      "Epoch 47/50\n",
      "4149/4149 [==============================] - 0s 100us/sample - loss: 0.6586 - acc: 0.6038 - val_loss: 0.7142 - val_acc: 0.5299\n",
      "Epoch 48/50\n",
      "4149/4149 [==============================] - 0s 98us/sample - loss: 0.6580 - acc: 0.6071 - val_loss: 0.7142 - val_acc: 0.5328\n",
      "Epoch 49/50\n",
      "4149/4149 [==============================] - 0s 97us/sample - loss: 0.6571 - acc: 0.6057 - val_loss: 0.7157 - val_acc: 0.5299\n",
      "Epoch 50/50\n",
      "4149/4149 [==============================] - 0s 91us/sample - loss: 0.6569 - acc: 0.6076 - val_loss: 0.7148 - val_acc: 0.5337\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential() \n",
    "model.add(Dense(16, input_shape = (30,)))\n",
    "model.add(Dense(16, activation = 'relu')) \n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "hist = model.fit(X_train_norm, y_train, epochs = 50, batch_size = 16, validation_split = .2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24e69ea6248>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5535851966075559"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction  = (model.predict(X_test_norm) > 0.5).astype(int)\n",
    "accuracy_score(y_test, prediction )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More changes to maybe have mutlinomial regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>test</th>\n",
       "      <th>china</th>\n",
       "      <th>stock_dif</th>\n",
       "      <th>stock_up</th>\n",
       "      <th>china_similarity</th>\n",
       "      <th>xi_similarity</th>\n",
       "      <th>...</th>\n",
       "      <th>border_similarity</th>\n",
       "      <th>president_similarity</th>\n",
       "      <th>congressman_similarity</th>\n",
       "      <th>people_similarity</th>\n",
       "      <th>korea_similarity</th>\n",
       "      <th>years_similarity</th>\n",
       "      <th>farmers_similarity</th>\n",
       "      <th>going_similarity</th>\n",
       "      <th>trade_similarity</th>\n",
       "      <th>never_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19813.079735</td>\n",
       "      <td>84534.203578</td>\n",
       "      <td>0.154282</td>\n",
       "      <td>0.465579</td>\n",
       "      <td>0.141149</td>\n",
       "      <td>0.036089</td>\n",
       "      <td>0.027067</td>\n",
       "      <td>0.435379</td>\n",
       "      <td>0.342459</td>\n",
       "      <td>0.526167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131029</td>\n",
       "      <td>0.219011</td>\n",
       "      <td>0.433023</td>\n",
       "      <td>0.121850</td>\n",
       "      <td>0.532239</td>\n",
       "      <td>0.265448</td>\n",
       "      <td>0.480092</td>\n",
       "      <td>0.101190</td>\n",
       "      <td>0.278569</td>\n",
       "      <td>0.102765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10384.792836</td>\n",
       "      <td>39760.567157</td>\n",
       "      <td>0.338861</td>\n",
       "      <td>0.275317</td>\n",
       "      <td>0.305913</td>\n",
       "      <td>0.186526</td>\n",
       "      <td>1.664811</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.067125</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041920</td>\n",
       "      <td>0.123907</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.065433</td>\n",
       "      <td>0.100870</td>\n",
       "      <td>0.058740</td>\n",
       "      <td>0.087544</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.058187</td>\n",
       "      <td>0.037853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-21.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.232626</td>\n",
       "      <td>-0.387025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165719</td>\n",
       "      <td>-0.344826</td>\n",
       "      <td>-0.512832</td>\n",
       "      <td>-0.399083</td>\n",
       "      <td>-0.674654</td>\n",
       "      <td>-0.309716</td>\n",
       "      <td>-0.054812</td>\n",
       "      <td>-0.584994</td>\n",
       "      <td>-0.273663</td>\n",
       "      <td>-0.408032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13430.500000</td>\n",
       "      <td>59651.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339754</td>\n",
       "      <td>0.536195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133036</td>\n",
       "      <td>0.147932</td>\n",
       "      <td>0.427395</td>\n",
       "      <td>0.131740</td>\n",
       "      <td>0.531322</td>\n",
       "      <td>0.264176</td>\n",
       "      <td>0.480649</td>\n",
       "      <td>0.091650</td>\n",
       "      <td>0.275898</td>\n",
       "      <td>0.089766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18169.500000</td>\n",
       "      <td>78726.500000</td>\n",
       "      <td>0.087170</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355121</td>\n",
       "      <td>0.553768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144455</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.449483</td>\n",
       "      <td>0.139732</td>\n",
       "      <td>0.549355</td>\n",
       "      <td>0.276733</td>\n",
       "      <td>0.497538</td>\n",
       "      <td>0.102501</td>\n",
       "      <td>0.288671</td>\n",
       "      <td>0.098628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24017.500000</td>\n",
       "      <td>102456.000000</td>\n",
       "      <td>0.348122</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.368255</td>\n",
       "      <td>0.567665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153815</td>\n",
       "      <td>0.385698</td>\n",
       "      <td>0.468053</td>\n",
       "      <td>0.147093</td>\n",
       "      <td>0.567102</td>\n",
       "      <td>0.288115</td>\n",
       "      <td>0.511316</td>\n",
       "      <td>0.114161</td>\n",
       "      <td>0.300712</td>\n",
       "      <td>0.108586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>131283.000000</td>\n",
       "      <td>574621.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521302</td>\n",
       "      <td>0.861641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285948</td>\n",
       "      <td>0.550976</td>\n",
       "      <td>0.671093</td>\n",
       "      <td>0.421210</td>\n",
       "      <td>0.762448</td>\n",
       "      <td>0.435014</td>\n",
       "      <td>0.681942</td>\n",
       "      <td>0.222980</td>\n",
       "      <td>0.503452</td>\n",
       "      <td>0.273554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       retweet_count  favorite_count     polarity  subjectivity         test  \\\n",
       "count    6484.000000     6484.000000  6484.000000   6484.000000  6484.000000   \n",
       "mean    19813.079735    84534.203578     0.154282      0.465579     0.141149   \n",
       "std     10384.792836    39760.567157     0.338861      0.275317     0.305913   \n",
       "min         2.000000        0.000000    -1.000000      0.000000    -1.000000   \n",
       "25%     13430.500000    59651.750000     0.000000      0.300000     0.000000   \n",
       "50%     18169.500000    78726.500000     0.087170      0.500000     0.086667   \n",
       "75%     24017.500000   102456.000000     0.348122      0.651583     0.325000   \n",
       "max    131283.000000   574621.000000     1.000000      1.000000     1.000000   \n",
       "\n",
       "             china    stock_dif     stock_up  china_similarity  xi_similarity  \\\n",
       "count  6484.000000  6484.000000  6484.000000       6484.000000    6484.000000   \n",
       "mean      0.036089     0.027067     0.435379          0.342459       0.526167   \n",
       "std       0.186526     1.664811     0.495845          0.067125       0.104200   \n",
       "min       0.000000   -21.750000     0.000000         -0.232626      -0.387025   \n",
       "25%       0.000000    -0.500000     0.000000          0.339754       0.536195   \n",
       "50%       0.000000     0.000000     0.000000          0.355121       0.553768   \n",
       "75%       0.000000     0.500000     1.000000          0.368255       0.567665   \n",
       "max       1.000000    22.250000     1.000000          0.521302       0.861641   \n",
       "\n",
       "       ...  border_similarity  president_similarity  congressman_similarity  \\\n",
       "count  ...        6484.000000           6484.000000             6484.000000   \n",
       "mean   ...           0.131029              0.219011                0.433023   \n",
       "std    ...           0.041920              0.123907                0.085100   \n",
       "min    ...          -0.165719             -0.344826               -0.512832   \n",
       "25%    ...           0.133036              0.147932                0.427395   \n",
       "50%    ...           0.144455              0.159800                0.449483   \n",
       "75%    ...           0.153815              0.385698                0.468053   \n",
       "max    ...           0.285948              0.550976                0.671093   \n",
       "\n",
       "       people_similarity  korea_similarity  years_similarity  \\\n",
       "count        6484.000000       6484.000000       6484.000000   \n",
       "mean            0.121850          0.532239          0.265448   \n",
       "std             0.065433          0.100870          0.058740   \n",
       "min            -0.399083         -0.674654         -0.309716   \n",
       "25%             0.131740          0.531322          0.264176   \n",
       "50%             0.139732          0.549355          0.276733   \n",
       "75%             0.147093          0.567102          0.288115   \n",
       "max             0.421210          0.762448          0.435014   \n",
       "\n",
       "       farmers_similarity  going_similarity  trade_similarity  \\\n",
       "count         6484.000000       6484.000000       6484.000000   \n",
       "mean             0.480092          0.101190          0.278569   \n",
       "std              0.087544          0.031397          0.058187   \n",
       "min             -0.054812         -0.584994         -0.273663   \n",
       "25%              0.480649          0.091650          0.275898   \n",
       "50%              0.497538          0.102501          0.288671   \n",
       "75%              0.511316          0.114161          0.300712   \n",
       "max              0.681942          0.222980          0.503452   \n",
       "\n",
       "       never_similarity  \n",
       "count       6484.000000  \n",
       "mean           0.102765  \n",
       "std            0.037853  \n",
       "min           -0.408032  \n",
       "25%            0.089766  \n",
       "50%            0.098628  \n",
       "75%            0.108586  \n",
       "max            0.273554  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>test</th>\n",
       "      <th>china</th>\n",
       "      <th>stock_dif</th>\n",
       "      <th>stock_up</th>\n",
       "      <th>china_similarity</th>\n",
       "      <th>xi_similarity</th>\n",
       "      <th>...</th>\n",
       "      <th>president_similarity</th>\n",
       "      <th>congressman_similarity</th>\n",
       "      <th>people_similarity</th>\n",
       "      <th>korea_similarity</th>\n",
       "      <th>years_similarity</th>\n",
       "      <th>farmers_similarity</th>\n",
       "      <th>going_similarity</th>\n",
       "      <th>trade_similarity</th>\n",
       "      <th>never_similarity</th>\n",
       "      <th>stock_movement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "      <td>6484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19813.079735</td>\n",
       "      <td>84534.203578</td>\n",
       "      <td>0.154282</td>\n",
       "      <td>0.465579</td>\n",
       "      <td>0.141149</td>\n",
       "      <td>0.036089</td>\n",
       "      <td>0.027067</td>\n",
       "      <td>0.435379</td>\n",
       "      <td>0.342459</td>\n",
       "      <td>0.526167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219011</td>\n",
       "      <td>0.433023</td>\n",
       "      <td>0.121850</td>\n",
       "      <td>0.532239</td>\n",
       "      <td>0.265448</td>\n",
       "      <td>0.480092</td>\n",
       "      <td>0.101190</td>\n",
       "      <td>0.278569</td>\n",
       "      <td>0.102765</td>\n",
       "      <td>0.019124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10384.792836</td>\n",
       "      <td>39760.567157</td>\n",
       "      <td>0.338861</td>\n",
       "      <td>0.275317</td>\n",
       "      <td>0.305913</td>\n",
       "      <td>0.186526</td>\n",
       "      <td>1.664811</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.067125</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123907</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.065433</td>\n",
       "      <td>0.100870</td>\n",
       "      <td>0.058740</td>\n",
       "      <td>0.087544</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.058187</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.777561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-21.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.232626</td>\n",
       "      <td>-0.387025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344826</td>\n",
       "      <td>-0.512832</td>\n",
       "      <td>-0.399083</td>\n",
       "      <td>-0.674654</td>\n",
       "      <td>-0.309716</td>\n",
       "      <td>-0.054812</td>\n",
       "      <td>-0.584994</td>\n",
       "      <td>-0.273663</td>\n",
       "      <td>-0.408032</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13430.500000</td>\n",
       "      <td>59651.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339754</td>\n",
       "      <td>0.536195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147932</td>\n",
       "      <td>0.427395</td>\n",
       "      <td>0.131740</td>\n",
       "      <td>0.531322</td>\n",
       "      <td>0.264176</td>\n",
       "      <td>0.480649</td>\n",
       "      <td>0.091650</td>\n",
       "      <td>0.275898</td>\n",
       "      <td>0.089766</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18169.500000</td>\n",
       "      <td>78726.500000</td>\n",
       "      <td>0.087170</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355121</td>\n",
       "      <td>0.553768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.449483</td>\n",
       "      <td>0.139732</td>\n",
       "      <td>0.549355</td>\n",
       "      <td>0.276733</td>\n",
       "      <td>0.497538</td>\n",
       "      <td>0.102501</td>\n",
       "      <td>0.288671</td>\n",
       "      <td>0.098628</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24017.500000</td>\n",
       "      <td>102456.000000</td>\n",
       "      <td>0.348122</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.368255</td>\n",
       "      <td>0.567665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385698</td>\n",
       "      <td>0.468053</td>\n",
       "      <td>0.147093</td>\n",
       "      <td>0.567102</td>\n",
       "      <td>0.288115</td>\n",
       "      <td>0.511316</td>\n",
       "      <td>0.114161</td>\n",
       "      <td>0.300712</td>\n",
       "      <td>0.108586</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>131283.000000</td>\n",
       "      <td>574621.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521302</td>\n",
       "      <td>0.861641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550976</td>\n",
       "      <td>0.671093</td>\n",
       "      <td>0.421210</td>\n",
       "      <td>0.762448</td>\n",
       "      <td>0.435014</td>\n",
       "      <td>0.681942</td>\n",
       "      <td>0.222980</td>\n",
       "      <td>0.503452</td>\n",
       "      <td>0.273554</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       retweet_count  favorite_count     polarity  subjectivity         test  \\\n",
       "count    6484.000000     6484.000000  6484.000000   6484.000000  6484.000000   \n",
       "mean    19813.079735    84534.203578     0.154282      0.465579     0.141149   \n",
       "std     10384.792836    39760.567157     0.338861      0.275317     0.305913   \n",
       "min         2.000000        0.000000    -1.000000      0.000000    -1.000000   \n",
       "25%     13430.500000    59651.750000     0.000000      0.300000     0.000000   \n",
       "50%     18169.500000    78726.500000     0.087170      0.500000     0.086667   \n",
       "75%     24017.500000   102456.000000     0.348122      0.651583     0.325000   \n",
       "max    131283.000000   574621.000000     1.000000      1.000000     1.000000   \n",
       "\n",
       "             china    stock_dif     stock_up  china_similarity  xi_similarity  \\\n",
       "count  6484.000000  6484.000000  6484.000000       6484.000000    6484.000000   \n",
       "mean      0.036089     0.027067     0.435379          0.342459       0.526167   \n",
       "std       0.186526     1.664811     0.495845          0.067125       0.104200   \n",
       "min       0.000000   -21.750000     0.000000         -0.232626      -0.387025   \n",
       "25%       0.000000    -0.500000     0.000000          0.339754       0.536195   \n",
       "50%       0.000000     0.000000     0.000000          0.355121       0.553768   \n",
       "75%       0.000000     0.500000     1.000000          0.368255       0.567665   \n",
       "max       1.000000    22.250000     1.000000          0.521302       0.861641   \n",
       "\n",
       "       ...  president_similarity  congressman_similarity  people_similarity  \\\n",
       "count  ...           6484.000000             6484.000000        6484.000000   \n",
       "mean   ...              0.219011                0.433023           0.121850   \n",
       "std    ...              0.123907                0.085100           0.065433   \n",
       "min    ...             -0.344826               -0.512832          -0.399083   \n",
       "25%    ...              0.147932                0.427395           0.131740   \n",
       "50%    ...              0.159800                0.449483           0.139732   \n",
       "75%    ...              0.385698                0.468053           0.147093   \n",
       "max    ...              0.550976                0.671093           0.421210   \n",
       "\n",
       "       korea_similarity  years_similarity  farmers_similarity  \\\n",
       "count       6484.000000       6484.000000         6484.000000   \n",
       "mean           0.532239          0.265448            0.480092   \n",
       "std            0.100870          0.058740            0.087544   \n",
       "min           -0.674654         -0.309716           -0.054812   \n",
       "25%            0.531322          0.264176            0.480649   \n",
       "50%            0.549355          0.276733            0.497538   \n",
       "75%            0.567102          0.288115            0.511316   \n",
       "max            0.762448          0.435014            0.681942   \n",
       "\n",
       "       going_similarity  trade_similarity  never_similarity  stock_movement  \n",
       "count       6484.000000       6484.000000       6484.000000     6484.000000  \n",
       "mean           0.101190          0.278569          0.102765        0.019124  \n",
       "std            0.031397          0.058187          0.037853        0.777561  \n",
       "min           -0.584994         -0.273663         -0.408032       -1.000000  \n",
       "25%            0.091650          0.275898          0.089766       -1.000000  \n",
       "50%            0.102501          0.288671          0.098628        0.000000  \n",
       "75%            0.114161          0.300712          0.108586        1.000000  \n",
       "max            0.222980          0.503452          0.273554        1.000000  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_multiclass = checkpoint\n",
    "down = np.where(tweets_multiclass.stock_dif < -0.25, -1, 0)\n",
    "up = np.where(tweets_multiclass.stock_dif > 0.25, 1, 0)\n",
    "tweets_multiclass['stock_movement'] = down + up\n",
    "\n",
    "tweets_multiclass.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_df = tweets_multiclass.drop(columns=['china', 'tariff', 'farmer', 'trade', 'war','text', 'stock_dif', 'stock_up', 'is_retweet', 'created_at'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy on Training Set: 0.5542702911123963\n",
      "Random Forest Accuracy on Test Set: 0.39707016191210487\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(multiclass_df.loc[:, multiclass_df.columns != 'stock_movement'], \n",
    "                                                         multiclass_df.stock_movement, test_size=0.2, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = multiclass_df.stock_movement)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFclassifier = RandomForestClassifier(max_depth = 7, n_estimators=250, max_features='sqrt').fit(X_train, y_train)\n",
    "random_forest_train_score = accuracy_score(RFclassifier.predict(X_train), y_train)\n",
    "random_forest_test_score = accuracy_score(RFclassifier.predict(X_test), y_test)\n",
    "print('Random Forest Accuracy on Training Set:', random_forest_train_score)\n",
    "print('Random Forest Accuracy on Test Set:', random_forest_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential() \n",
    "model.add(Dense(16, input_shape = (29,)))\n",
    "model.add(Dense(16, activation = 'relu')) \n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "hist = model.fit(X_train_norm, y_train, epochs = 50, batch_size = 16, validation_split = .2 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
