{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn as sk \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('data/preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = temp.drop(columns = ['Unnamed: 0', 'text', 'created_at','is_retweet', 'stock_dif', 'preproc', 'keywords'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['stock_up']\n",
    "x = data.drop(columns = ['stock_up'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the data \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape = (154,), activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4069 samples, validate on 1018 samples\n",
      "Epoch 1/10\n",
      "4069/4069 [==============================] - 1s 209us/sample - loss: 0.7145 - acc: 0.5392 - val_loss: 0.7091 - val_acc: 0.5147\n",
      "Epoch 2/10\n",
      "4069/4069 [==============================] - 0s 65us/sample - loss: 0.6718 - acc: 0.5812 - val_loss: 0.7067 - val_acc: 0.5275\n",
      "Epoch 3/10\n",
      "4069/4069 [==============================] - 0s 67us/sample - loss: 0.6530 - acc: 0.6154 - val_loss: 0.7087 - val_acc: 0.5334\n",
      "Epoch 4/10\n",
      "4069/4069 [==============================] - 0s 67us/sample - loss: 0.6352 - acc: 0.6456 - val_loss: 0.7121 - val_acc: 0.5354\n",
      "Epoch 5/10\n",
      "4069/4069 [==============================] - 0s 72us/sample - loss: 0.6167 - acc: 0.6626 - val_loss: 0.7221 - val_acc: 0.5059\n",
      "Epoch 6/10\n",
      "4069/4069 [==============================] - 0s 72us/sample - loss: 0.5983 - acc: 0.6943 - val_loss: 0.7381 - val_acc: 0.5118\n",
      "Epoch 7/10\n",
      "4069/4069 [==============================] - 0s 67us/sample - loss: 0.5791 - acc: 0.7085 - val_loss: 0.7477 - val_acc: 0.5147\n",
      "Epoch 8/10\n",
      "4069/4069 [==============================] - 0s 67us/sample - loss: 0.5566 - acc: 0.7235 - val_loss: 0.7596 - val_acc: 0.5128\n",
      "Epoch 9/10\n",
      "4069/4069 [==============================] - 0s 67us/sample - loss: 0.5331 - acc: 0.7491 - val_loss: 0.7789 - val_acc: 0.5147\n",
      "Epoch 10/10\n",
      "4069/4069 [==============================] - 0s 66us/sample - loss: 0.5089 - acc: 0.7636 - val_loss: 0.8138 - val_acc: 0.5088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2aae3b18e88>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, validation_split = .2, batch_size = 32, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_scaled)\n",
    "prediction = pd.DataFrame(predictions, index = X_test.index)\n",
    "dates = temp.iloc[X_test.index]['created_at']\n",
    "neural_predict = pd.concat([prediction, dates], axis = 1)\n",
    "neural_predict.to_csv('data/neural_prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
