{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from spacy.lang.en.examples import sentences \n",
    "import spacy as sp\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline\n",
    "# import zipfile\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('data/TrumpTweetsCleaned.csv')\n",
    "#09-11-2017 to 02-16-2018\n",
    "before = pd.to_datetime('2017-09-11')\n",
    "after = pd.to_datetime('2018-02-16')\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (James)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing functions for the dataframe\n",
    "def delete_punct(text):\n",
    "    '''\n",
    "    removes special characters from the document\n",
    "    '''\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct \n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    removes all stopwords according to the pre-built english dictionary of known stopwords\n",
    "    '''\n",
    "    words = [w for w in text if w not in set(stopwords.words('english'))]\n",
    "    return words\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    returns the lemmas of each word in the document\n",
    "    '''\n",
    "    lemmatized = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lemmatized\n",
    "def joiner(text):\n",
    "    '''\n",
    "    joins the comma-separated list into one string\n",
    "    '''\n",
    "    joined = \" \".join([i for i in text])\n",
    "    return joined\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_pipeline(tweets):\n",
    "    '''\n",
    "    The preprocessing pipline applied to each document in the dataframe\n",
    "    '''\n",
    "    tweets['preproc'] = tweets['text'].apply(lambda x: delete_punct(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: remove_stopwords(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: lemmatize(x))\n",
    "    tweets['preproc'] = tweets['preproc'].apply(lambda x: joiner(x))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = preproc_pipeline(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting sk-learn's TF-IDF Vectorizer to our dataframe and returning list of most important keywords\n",
    "docs = tweets['preproc'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features = 2500, min_df = 10, max_df = .9)\n",
    "tfidf2 = TfidfVectorizer(max_features = 150, min_df = 160, max_df = .9)\n",
    "X = tfidf.fit_transform(docs).toarray()\n",
    "feature_names = tfidf.get_feature_names()\n",
    "X2 = tfidf2.fit_transform(docs).toarray()\n",
    "features_names2 = tfidf2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(l1,l2):\n",
    "    '''\n",
    "    finding intersection of elements between two lists\n",
    "    '''\n",
    "    return list(set(l1) & set(l2))\n",
    "def term_extract(df):\n",
    "    '''\n",
    "    Will return a column of important keywords in each document determined by TF-IDF \n",
    "    Sentiment Analysis via Textblob will only be run on these keywords \n",
    "    '''\n",
    "    df['keywords'] = df['preproc'].apply(lambda x: intersection(x.split(' '), feature_names))\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: joiner(x))\n",
    "    return df\n",
    "tweets = term_extract(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['polarity'] = [TextBlob(tweets['keywords'].values[i]).polarity for i in range(len(tweets))]\n",
    "tweets['subjectivity'] = [TextBlob(tweets['keywords'].values[i]).subjectivity for i in range(len(tweets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns \n",
    "# tweets  = tweets.drop(columns = ['text', 'preproc'])\n",
    "tweets = tweets.replace('', np.nan)\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_stock_viable = pd.read_csv('data/tweets_stock_viable_5')\n",
    "# tweets_stock_viable['keywords'] = tweets_stock_viable['keywords'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# tokenize sentences in corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in tweets['keywords']]\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(df):\n",
    "    columns = []\n",
    "    for keyword in features_names2:\n",
    "        icols = []\n",
    "        for i in df:\n",
    "            temp = []\n",
    "            list_ = i.split()\n",
    "            for j in list_:\n",
    "                if j in w2v_model.wv.vocab:\n",
    "                    temp.append(w2v_model.wv.similarity(j, 'china'))\n",
    "            icols.append(max(temp))\n",
    "        columns.append(icols)\n",
    "    return columns\n",
    "columns = word_similarity(tweets['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_similarity = pd.DataFrame(columns).T\n",
    "word_similarity.columns = features_names2\n",
    "word_similarity.index = tweets.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = pd.concat([tweets, word_similarity], axis = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = pd.read_csv('data/SP500_intraday.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['Time_pd'] = pd.to_timedelta(stock['Time']+':00')\n",
    "stock['Date_time'] = pd.to_datetime(pd.to_datetime(stock['Date'])+ pd.to_timedelta(stock['Time_pd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_time = stock['Date_time'].iloc[-1]\n",
    "earliest_time = stock['Date_time'][0] \n",
    "temp = preprocessed[(preprocessed['created_at'] > earliest_time) & (preprocessed['created_at'] < latest_time)]\n",
    "temp = temp.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>preproc</th>\n",
       "      <th>keywords</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>administration</th>\n",
       "      <th>...</th>\n",
       "      <th>witch</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>äôs</th>\n",
       "      <th>äôt</th>\n",
       "      <th>äù</th>\n",
       "      <th>üá</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The debates especially the second and third pl...</td>\n",
       "      <td>2016-11-13 18:46:00</td>\n",
       "      <td>23410.0</td>\n",
       "      <td>113207.0</td>\n",
       "      <td>False</td>\n",
       "      <td>debate especially second third plus speech int...</td>\n",
       "      <td>especially win supporter gave speech third lar...</td>\n",
       "      <td>0.402857</td>\n",
       "      <td>0.395714</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "      <td>0.135235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If the election were based on total popular vo...</td>\n",
       "      <td>2016-11-15 13:34:00</td>\n",
       "      <td>49623.0</td>\n",
       "      <td>179219.0</td>\n",
       "      <td>False</td>\n",
       "      <td>election based total popular vote would campai...</td>\n",
       "      <td>total even election based campaigned popular c...</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "      <td>0.119161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Electoral College is actually genius in th...</td>\n",
       "      <td>2016-11-15 13:40:00</td>\n",
       "      <td>39125.0</td>\n",
       "      <td>128083.0</td>\n",
       "      <td>False</td>\n",
       "      <td>electoral college actually genius brings state...</td>\n",
       "      <td>different state one college actually including...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>0.143061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very organized process taking place as I decid...</td>\n",
       "      <td>2016-11-16 02:55:00</td>\n",
       "      <td>27544.0</td>\n",
       "      <td>119611.0</td>\n",
       "      <td>False</td>\n",
       "      <td>organized process taking place decide cabinet ...</td>\n",
       "      <td>one taking place process many cabinet position...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "      <td>0.252220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am not trying to get \"top level security cle...</td>\n",
       "      <td>2016-11-16 11:28:00</td>\n",
       "      <td>31102.0</td>\n",
       "      <td>99921.0</td>\n",
       "      <td>False</td>\n",
       "      <td>trying get top level security clearance child ...</td>\n",
       "      <td>trying false top news level get child security...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.265493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9369</th>\n",
       "      <td>The Amazon Washington Post and three lowlife r...</td>\n",
       "      <td>2019-11-07 15:27:00</td>\n",
       "      <td>24007.0</td>\n",
       "      <td>83333.0</td>\n",
       "      <td>False</td>\n",
       "      <td>amazon washington post three lowlife reporter ...</td>\n",
       "      <td>barr bill source matt another amazon post stor...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "      <td>0.203453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9370</th>\n",
       "      <td>The Radical Left Dems and LameStream Media are...</td>\n",
       "      <td>2019-11-07 15:41:00</td>\n",
       "      <td>18328.0</td>\n",
       "      <td>72619.0</td>\n",
       "      <td>False</td>\n",
       "      <td>radical left dems lamestream medium trying mak...</td>\n",
       "      <td>hoax impeachment trying dems republican lamest...</td>\n",
       "      <td>0.161174</td>\n",
       "      <td>0.349053</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "      <td>0.196683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9371</th>\n",
       "      <td>Stock Market up big today. A New Record. Enjoy!</td>\n",
       "      <td>2019-11-07 15:43:00</td>\n",
       "      <td>20211.0</td>\n",
       "      <td>114187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>stock market big today new record enjoy</td>\n",
       "      <td>market stock record today big new enjoy</td>\n",
       "      <td>0.178788</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "      <td>0.099571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9372</th>\n",
       "      <td>STATEMENT FROM PRESIDENT DONALD J. TRUMP https...</td>\n",
       "      <td>2019-11-08 00:08:00</td>\n",
       "      <td>31818.0</td>\n",
       "      <td>110993.0</td>\n",
       "      <td>False</td>\n",
       "      <td>statement president donald j trump httpstcoekt...</td>\n",
       "      <td>statement donald president trump</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "      <td>0.075596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9373</th>\n",
       "      <td>https://t.co/z0I7wBsgTP</td>\n",
       "      <td>2019-11-08 03:08:00</td>\n",
       "      <td>33174.0</td>\n",
       "      <td>114807.0</td>\n",
       "      <td>False</td>\n",
       "      <td>httpstcoz 0i7wbsgtp</td>\n",
       "      <td>httpstcoz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "      <td>-0.141193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9374 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text          created_at  \\\n",
       "0     The debates especially the second and third pl... 2016-11-13 18:46:00   \n",
       "1     If the election were based on total popular vo... 2016-11-15 13:34:00   \n",
       "2     The Electoral College is actually genius in th... 2016-11-15 13:40:00   \n",
       "3     Very organized process taking place as I decid... 2016-11-16 02:55:00   \n",
       "4     I am not trying to get \"top level security cle... 2016-11-16 11:28:00   \n",
       "...                                                 ...                 ...   \n",
       "9369  The Amazon Washington Post and three lowlife r... 2019-11-07 15:27:00   \n",
       "9370  The Radical Left Dems and LameStream Media are... 2019-11-07 15:41:00   \n",
       "9371    Stock Market up big today. A New Record. Enjoy! 2019-11-07 15:43:00   \n",
       "9372  STATEMENT FROM PRESIDENT DONALD J. TRUMP https... 2019-11-08 00:08:00   \n",
       "9373                            https://t.co/z0I7wBsgTP 2019-11-08 03:08:00   \n",
       "\n",
       "      retweet_count  favorite_count  is_retweet  \\\n",
       "0           23410.0        113207.0       False   \n",
       "1           49623.0        179219.0       False   \n",
       "2           39125.0        128083.0       False   \n",
       "3           27544.0        119611.0       False   \n",
       "4           31102.0         99921.0       False   \n",
       "...             ...             ...         ...   \n",
       "9369        24007.0         83333.0       False   \n",
       "9370        18328.0         72619.0       False   \n",
       "9371        20211.0        114187.0       False   \n",
       "9372        31818.0        110993.0       False   \n",
       "9373        33174.0        114807.0       False   \n",
       "\n",
       "                                                preproc  \\\n",
       "0     debate especially second third plus speech int...   \n",
       "1     election based total popular vote would campai...   \n",
       "2     electoral college actually genius brings state...   \n",
       "3     organized process taking place decide cabinet ...   \n",
       "4     trying get top level security clearance child ...   \n",
       "...                                                 ...   \n",
       "9369  amazon washington post three lowlife reporter ...   \n",
       "9370  radical left dems lamestream medium trying mak...   \n",
       "9371            stock market big today new record enjoy   \n",
       "9372  statement president donald j trump httpstcoekt...   \n",
       "9373                                httpstcoz 0i7wbsgtp   \n",
       "\n",
       "                                               keywords  polarity  \\\n",
       "0     especially win supporter gave speech third lar...  0.402857   \n",
       "1     total even election based campaigned popular c...  0.258333   \n",
       "2     different state one college actually including...  0.066667   \n",
       "3     one taking place process many cabinet position...  0.500000   \n",
       "4     trying false top news level get child security...  0.050000   \n",
       "...                                                 ...       ...   \n",
       "9369  barr bill source matt another amazon post stor... -0.500000   \n",
       "9370  hoax impeachment trying dems republican lamest...  0.161174   \n",
       "9371            market stock record today big new enjoy  0.178788   \n",
       "9372                   statement donald president trump  0.000000   \n",
       "9373                                          httpstcoz  0.000000   \n",
       "\n",
       "      subjectivity  administration  ...     witch      work   working  \\\n",
       "0         0.395714        0.135235  ...  0.135235  0.135235  0.135235   \n",
       "1         0.745833        0.119161  ...  0.119161  0.119161  0.119161   \n",
       "2         0.300000        0.143061  ...  0.143061  0.143061  0.143061   \n",
       "3         0.500000        0.252220  ...  0.252220  0.252220  0.252220   \n",
       "4         0.550000        0.265493  ...  0.265493  0.265493  0.265493   \n",
       "...            ...             ...  ...       ...       ...       ...   \n",
       "9369      1.000000        0.203453  ...  0.203453  0.203453  0.203453   \n",
       "9370      0.349053        0.196683  ...  0.196683  0.196683  0.196683   \n",
       "9371      0.351515        0.099571  ...  0.099571  0.099571  0.099571   \n",
       "9372      0.000000        0.075596  ...  0.075596  0.075596  0.075596   \n",
       "9373      0.000000       -0.141193  ... -0.141193 -0.141193 -0.141193   \n",
       "\n",
       "         world     would      year       äôs       äôt        äù        üá  \n",
       "0     0.135235  0.135235  0.135235  0.135235  0.135235  0.135235  0.135235  \n",
       "1     0.119161  0.119161  0.119161  0.119161  0.119161  0.119161  0.119161  \n",
       "2     0.143061  0.143061  0.143061  0.143061  0.143061  0.143061  0.143061  \n",
       "3     0.252220  0.252220  0.252220  0.252220  0.252220  0.252220  0.252220  \n",
       "4     0.265493  0.265493  0.265493  0.265493  0.265493  0.265493  0.265493  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9369  0.203453  0.203453  0.203453  0.203453  0.203453  0.203453  0.203453  \n",
       "9370  0.196683  0.196683  0.196683  0.196683  0.196683  0.196683  0.196683  \n",
       "9371  0.099571  0.099571  0.099571  0.099571  0.099571  0.099571  0.099571  \n",
       "9372  0.075596  0.075596  0.075596  0.075596  0.075596  0.075596  0.075596  \n",
       "9373 -0.141193 -0.141193 -0.141193 -0.141193 -0.141193 -0.141193 -0.141193  \n",
       "\n",
       "[9374 rows x 159 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edit the minute after\n",
    "time_min = 5\n",
    "time_after = temp['created_at'] + pd.to_timedelta(time_min, unit = 'm')\n",
    "time_of = temp['created_at']\n",
    "good_time_after = sorted(list(set(stock['Date_time']) & set(list(time_after))))\n",
    "x = np.arange(0,len(good_time_after))\n",
    "tweets_stock_viable = temp.iloc[np.concatenate([np.where(time_after == good_time_after[x])[0] for x in x])]\n",
    "\n",
    "time_after2 = tweets_stock_viable['created_at'] + pd.to_timedelta(time_min, unit = 'm')\n",
    "time_of2 = tweets_stock_viable['created_at']\n",
    "x = time_of2\n",
    "\n",
    "bad_var = [stock[stock['Date_time'] == time_of2[i]]['Open'].values\n",
    "             for i in tweets_stock_viable.index]\n",
    "bad = np.where(pd.DataFrame(bad_var, index = tweets_stock_viable.index).isna())[0]\n",
    "tweets_stock_viable = tweets_stock_viable.drop(tweets_stock_viable.index[bad])\n",
    "stock_df = [stock[stock['Date_time'] == time_after2[i]]['Close'].values - stock[stock['Date_time'] == time_of2[i]]['Open'].values\n",
    "             for i in x.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable['stock_dif'] = pd.DataFrame(np.concatenate(stock_df), index= tweets_stock_viable.index)\n",
    "length = len(tweets_stock_viable)\n",
    "tweets_stock_viable['stock_up'] = [1 if tweets_stock_viable['stock_dif'][i] > 0 else 0 for i in tweets_stock_viable.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable = tweets_stock_viable.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_stock_viable.to_csv('data/preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim\n",
    "vocab = list(word_similarity) + ['xi', 'farm', 'farmer', 'deficit', 'intellectual', 'property']\n",
    "X = w2v_model[vocab]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "w2v_df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(w2v_df['x'], w2v_df['y'])\n",
    "ax.set_title('Dimension Reduced 2D Visualization of Word Similarities from Gensim Word2Vec Model')\n",
    "\n",
    "for word, pos in w2v_df.iterrows():\n",
    "    ax.annotate(word, pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
